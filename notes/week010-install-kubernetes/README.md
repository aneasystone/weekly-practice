# WEEK010 - Kubernetes å®‰è£…å°è®°

Kubernetes é›†ç¾¤ç¯å¢ƒçš„å®‰è£…æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦è€ƒè™‘ç½‘ç»œã€å­˜å‚¨ç­‰ä¸€ç³»åˆ—çš„é—®é¢˜ï¼Œåœ¨è¿™ç¯‡ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬å…ˆå­¦ä¹ ä½¿ç”¨ kind æˆ– minikube å®‰è£…å•æœºç¯å¢ƒï¼Œåœ¨å¯¹ Kubernetes çš„ç»„ä»¶å’ŒåŸºæœ¬æ¦‚å¿µæœ‰ä¸€å®šè®¤è¯†ä¹‹åï¼Œå†å°è¯•éƒ¨ç½²é›†ç¾¤ç¯å¢ƒã€‚

## å®‰è£… kubectl 

åœ¨å®‰è£… Kubernetes ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… `kubectl`ï¼Œè¿™æ˜¯ Kubernetes çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œç”¨æ¥åœ¨ Kubernetes é›†ç¾¤ä¸Šè¿è¡Œå‘½ä»¤ï¼Œä½ å¯ä»¥ä½¿ç”¨ `kubectl` æ¥éƒ¨ç½²åº”ç”¨ã€ç›‘æµ‹å’Œç®¡ç†é›†ç¾¤èµ„æºä»¥åŠæŸ¥çœ‹æ—¥å¿—ã€‚å®‰è£… `kubectl` æœ€ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨ `curl` å‘½ä»¤ï¼Œé¦–å…ˆæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ä¸‹è½½ `kubectl`ï¼š

```
$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
```

ç„¶åå°† `kubectl` å®‰è£…åˆ° `/usr/local/bin` ç›®å½•ï¼š

```
$ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
```

`install` å’Œ `cp` å‘½ä»¤ç±»ä¼¼ï¼Œéƒ½å¯ä»¥å°†æ–‡ä»¶æˆ–ç›®å½•æ‹·è´åˆ°æŒ‡å®šçš„åœ°æ–¹ï¼Œä¸è¿‡ `install` å…è®¸ä½ æ§åˆ¶æ–‡ä»¶çš„å±æ€§ã€‚`-o, --owner` å‚æ•°ç”¨æ¥è®¾ç½®æ‰€æœ‰è€…ï¼Œ`-g, --group` å‚æ•°ç”¨æ¥è®¾ç½®ç»„ï¼Œ`-m, --mode` ç±»ä¼¼äº chmod çš„è®¾å®šæ–‡ä»¶æƒé™æ¨¡å¼ã€‚

å®‰è£…å®Œæˆåï¼Œè¿è¡Œ `kubectl version` æŸ¥çœ‹ç‰ˆæœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼š

```
[root@localhost ~]# kubectl version --client --output=json
{
  "clientVersion": {
    "major": "1",
    "minor": "24",
    "gitVersion": "v1.24.0",
    "gitCommit": "4ce5a8954017644c5420bae81d72b09b735c21f0",
    "gitTreeState": "clean",
    "buildDate": "2022-05-03T13:46:05Z",
    "goVersion": "go1.18.1",
    "compiler": "gc",
    "platform": "linux/amd64"
  },
  "kustomizeVersion": "v4.5.4"
}
```

ç”±äºæ­¤æ—¶è¿˜æ²¡æœ‰å®‰è£… Kubernetesï¼Œæ‰€ä»¥ä½¿ç”¨ `--client` ä»…æ˜¾ç¤ºå®¢æˆ·ç«¯çš„ç‰ˆæœ¬ã€‚

## ä½¿ç”¨ kind å®‰è£… Kubernetes

[`kind`](https://kind.sigs.k8s.io/) æ˜¯ Kubernetes IN Docker çš„ç®€å†™ï¼Œæ˜¯ä¸€ä¸ªä½¿ç”¨ Docker å®¹å™¨ä½œä¸º Nodesï¼Œåœ¨æœ¬åœ°åˆ›å»ºå’Œè¿è¡Œ Kubernetes é›†ç¾¤çš„å·¥å…·ã€‚é€‚ç”¨äºåœ¨æœ¬æœºåˆ›å»º Kubernetes é›†ç¾¤ç¯å¢ƒè¿›è¡Œå¼€å‘å’Œæµ‹è¯•ã€‚

### å®‰è£… kind

å’Œå®‰è£… `kubectl` ç±»ä¼¼ï¼Œé¦–å…ˆä½¿ç”¨ `curl` ä¸‹è½½ï¼š

```
$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.12.0/kind-linux-amd64
```

å†ä½¿ç”¨ `install` å®‰è£…ï¼š

```
$ sudo install -o root -g root -m 0755 kind /usr/local/bin/kind
```

ä½¿ç”¨ `kind --help` æŸ¥çœ‹å¸®åŠ©ï¼š

```
[root@localhost ~]# kind --help
kind creates and manages local Kubernetes clusters using Docker container 'nodes'

Usage:
  kind [command]

Available Commands:
  build       Build one of [node-image]
  completion  Output shell completion code for the specified shell (bash, zsh or fish)
  create      Creates one of [cluster]
  delete      Deletes one of [cluster]
  export      Exports one of [kubeconfig, logs]
  get         Gets one of [clusters, nodes, kubeconfig]
  help        Help about any command
  load        Loads images into nodes
  version     Prints the kind CLI version

Flags:
  -h, --help              help for kind
      --loglevel string   DEPRECATED: see -v instead
  -q, --quiet             silence all stderr output
  -v, --verbosity int32   info log verbosity, higher value produces more output
      --version           version for kind

Use "kind [command] --help" for more information about a command.
```

### åˆ›å»º Kubernetes é›†ç¾¤

ä½¿ç”¨ç®€å•çš„ä¸€å¥å‘½ä»¤ `kind create cluster` å°±å¯ä»¥åœ¨æœ¬åœ°åˆ›å»ºä¸€æ•´å¥— Kubernetes é›†ç¾¤ï¼Œè¿™æ ·çš„ç¯å¢ƒç”¨äºå®éªŒå†åˆé€‚ä¸è¿‡ï¼š

```
[root@localhost ~]# kind create cluster
Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.23.4) ğŸ–¼ 
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Thanks for using kind! ğŸ˜Š
```

æ­¤æ—¶å†è¿è¡Œ `kubectl version` å‘½ä»¤ï¼Œå°±å¯ä»¥çœ‹åˆ° Kubernetes æœåŠ¡ç«¯çš„ä¿¡æ¯äº†ï¼š

```
[root@localhost ~]# kubectl version --output=json
{
  "clientVersion": {
    "major": "1",
    "minor": "24",
    "gitVersion": "v1.24.0",
    "gitCommit": "4ce5a8954017644c5420bae81d72b09b735c21f0",
    "gitTreeState": "clean",
    "buildDate": "2022-05-03T13:46:05Z",
    "goVersion": "go1.18.1",
    "compiler": "gc",
    "platform": "linux/amd64"
  },
  "kustomizeVersion": "v4.5.4",
  "serverVersion": {
    "major": "1",
    "minor": "23",
    "gitVersion": "v1.23.4",
    "gitCommit": "e6c093d87ea4cbb530a7b2ae91e54c0842d8308a",
    "gitTreeState": "clean",
    "buildDate": "2022-03-06T21:32:53Z",
    "goVersion": "go1.17.7",
    "compiler": "gc",
    "platform": "linux/amd64"
  }
}
```

ä½¿ç”¨ `docker ps` å¯ä»¥çœ‹åˆ°ä¸€ä¸ªåä¸º `kind-control-plane` çš„å®¹å™¨ï¼Œä»–æš´éœ²å‡ºæ¥çš„ç«¯å£ `127.0.0.1:45332` å°±æ˜¯æˆ‘ä»¬ `kubectl` è®¿é—®çš„ç«¯å£ã€‚

```
[root@localhost ~]# docker ps
CONTAINER ID   IMAGE                  COMMAND                  CREATED       STATUS       PORTS                       NAMES
2d2f2ed13eaa   kindest/node:v1.23.4   "/usr/local/bin/entrâ€¦"   2 hours ago   Up 2 hours   127.0.0.1:45332->6443/tcp   kind-control-plane
```

`kind` å°†æ•´ä¸ª Kubernetes ç»„ä»¶å†…ç½®åœ¨ `kindest/node` é•œåƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨è¯¥é•œåƒåˆ›å»ºå¤šä¸ª Kubernetes é›†ç¾¤ã€‚é»˜è®¤åˆ›å»ºçš„é›†ç¾¤åä¸º `kind`ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `--name` å‚æ•°æŒ‡å®šé›†ç¾¤åï¼š

```
[root@localhost ~]# kind create cluster --name kind-2
```

è·å–é›†ç¾¤åˆ—è¡¨ï¼š

```
[root@localhost ~]# kind get clusters
kind
kind2
```

ä½¿ç”¨ `kubectl cluster-info` åˆ‡æ¢é›†ç¾¤ï¼š

```
kubectl cluster-info --context kind-kind
kubectl cluster-info --context kind-kind-2
```

æˆ‘ä»¬ä½¿ç”¨ `docker exec` è¿›å…¥å®¹å™¨å†…éƒ¨çœ‹çœ‹ï¼š

```
[root@localhost ~]# docker exec -it 2d2 bash
root@kind-control-plane:/# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.1  16544  1084 ?        Ss   09:43   0:00 /sbin/init
root         192  0.0  0.1  19448  1504 ?        S<s  09:44   0:00 /lib/systemd/systemd-journald
root         204  3.3  2.2 1437696 23204 ?       Ssl  09:44   4:09 /usr/local/bin/containerd
root         310  0.0  0.3 713276  3600 ?        Sl   09:44   0:02 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 6e308f31e4045e7f5e3f8ab7
root         317  0.0  0.3 713276  3428 ?        Sl   09:44   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 9de51bcebdf67ce484709b90
root         351  0.0  0.3 713276  3432 ?        Sl   09:44   0:02 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b0094313aab3af65958f7a74
root         363  0.0  0.4 713276  4060 ?        Sl   09:44   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 12136290af44076c5b5faa19
root         483  3.1  4.1 11214772 41872 ?      Ssl  09:44   3:56 etcd --advertise-client-urls=https://172.18.0.2:2379 --cert-file=/etc/kubernetes/pki/
root         562  6.5 18.7 1056224 190292 ?      Ssl  09:44   8:08 kube-apiserver --advertise-address=172.18.0.2 --allow-privileged=true --authorization
root         651  3.1  4.3 1402784 44492 ?       Ssl  09:44   3:55 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kube
root         899  0.0  0.3 713276  3284 ?        Sl   09:46   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 383bf2636b6a0b1e14b8cd08
root         915  0.0  0.3 713020  3596 ?        Sl   09:46   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id b0fc9bc1eaf15846855b4e5e
root         983  0.0  0.7 733188  7908 ?        Ssl  09:46   0:04 /bin/kindnetd
root        1023  0.0  1.0 748152 11136 ?        Ssl  09:46   0:04 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-overrid
root        1234  2.9  4.1 767820 42316 ?        Ssl  09:47   3:34 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/k
root        1274  0.4  1.9 754000 19516 ?        Ssl  09:47   0:34 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authoriza
root        1367  0.0  0.3 713020  3772 ?        Sl   09:47   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id dc694d8f939cfec4277911fe
root        1371  0.0  0.4 713276  4848 ?        Sl   09:47   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 41b361804234b5c0fc353ff6
root        1393  0.0  0.3 713276  4044 ?        Sl   09:47   0:01 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id e4befc1237963effcb2a594b
root        1499  0.1  1.1 750568 11884 ?        Ssl  09:47   0:12 /coredns -conf /etc/coredns/Corefile
root        1526  0.1  1.1 750568 11772 ?        Ssl  09:47   0:13 /coredns -conf /etc/coredns/Corefile
root        2904  0.2  0.1   4580  1040 pts/1    Ss   11:47   0:00 bash
root        2980  0.6  0.6 136664  6392 ?        Ssl  11:48   0:00 local-path-provisioner --debug start --helper-image k8s.gcr.io/build-image/debian-bas
root        3010  0.0  0.1   6900  1420 pts/1    R+   11:48   0:00 ps aux
```

å¯ä»¥çœ‹åˆ°è¿™äº›è¿›ç¨‹ï¼š

* kindnetd - *ä¸€æ¬¾ç®€å•çš„ CNI æ’ä»¶*
* containerd - *ä½¿ç”¨ containerd ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ï¼Œå¼ƒç”¨ Dockershim å¯¹ kind æ²¡æœ‰å½±å“*
* containerd-shim-runc-v2
* pause
* coredns - *ä¸ºé›†ç¾¤æä¾› DNS å’ŒæœåŠ¡å‘ç°çš„åŠŸèƒ½*
* etcd - *æœåŠ¡å‘ç°çš„åç«¯ï¼Œå¹¶å­˜å‚¨é›†ç¾¤çŠ¶æ€å’Œé…ç½®*
* kubelet - *è¿è¡Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ä»£ç†ï¼Œç”¨æ¥å¤„ç† Master èŠ‚ç‚¹ä¸‹å‘åˆ°æœ¬èŠ‚ç‚¹çš„ä»»åŠ¡*
* kube-apiserver - *æä¾›é›†ç¾¤ç®¡ç†çš„ REST API æ¥å£ï¼Œæ˜¯æ¨¡å—ä¹‹é—´çš„æ•°æ®äº¤äº’å’Œé€šä¿¡çš„æ¢çº½ï¼Œåªæœ‰ apiserver èƒ½è®¿é—® etcd*
* kube-proxy - *å®ç° Kubernetes Service çš„é€šä¿¡ä¸è´Ÿè½½å‡è¡¡*
* kube-controller-manager - *æ˜¯ Kubernetes çš„å¤§è„‘ï¼Œå®ƒé€šè¿‡ apiserver ç›‘æ§æ•´ä¸ªé›†ç¾¤çš„çŠ¶æ€ï¼Œå¹¶ç¡®ä¿é›†ç¾¤å¤„äºé¢„æœŸçš„å·¥ä½œçŠ¶æ€*
* kube-scheduler - *è´Ÿè´£åˆ†é…è°ƒåº¦ Pod åˆ°é›†ç¾¤å†…çš„èŠ‚ç‚¹ä¸Šï¼Œå®ƒç›‘å¬ apiserverï¼ŒæŸ¥è¯¢è¿˜æœªåˆ†é… Node çš„ Podï¼Œç„¶åæ ¹æ®è°ƒåº¦ç­–ç•¥ä¸ºè¿™äº› Pod åˆ†é…èŠ‚ç‚¹*
* local-path-provisioner - *æœ¬åœ°æŒä¹…åŒ–å­˜å‚¨*

## ä½¿ç”¨ minikube å®‰è£… Kubernetes

[minikube](https://minikube.sigs.k8s.io/docs/) æ˜¯ç”± Google å‘å¸ƒçš„ä¸€æ¬¾è½»é‡çº§å·¥å…·ï¼Œè®©å¼€å‘è€…å¯ä»¥åœ¨æœ¬æœºä¸Šè½»æ˜“è¿è¡Œä¸€ä¸ª Kubernetes é›†ç¾¤ï¼Œå¿«é€Ÿä¸Šæ‰‹ Kubernetes çš„æŒ‡ä»¤ä¸ç¯å¢ƒã€‚`minikube` ä¼šåœ¨æœ¬æœºè¿è¡Œä¸€ä¸ªå®¹å™¨æˆ–è™šæ‹Ÿæœºï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªå®¹å™¨æˆ–è™šæ‹Ÿæœºä¸­å¯åŠ¨ä¸€ä¸ª single-node Kubernetes é›†ç¾¤ï¼Œå®ƒä¸æ”¯æŒ HAï¼Œä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚

### å®‰è£… minikube

`minikube` çš„å®‰è£…ä¹Ÿå’Œä¸Šé¢çš„ `kind` å’Œ `kubectl` ä¸€æ ·ï¼Œå…ˆä½¿ç”¨ `curl` ä¸‹è½½ï¼š

```
$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
```

å†é€šè¿‡ `install` å°†å…¶å®‰è£…åˆ° `/usr/local/bin` ç›®å½•ï¼š

```
$ sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

### åˆ›å»º Kubernetes é›†ç¾¤

ä½¿ç”¨ `minikube` åˆ›å»º Kubernetes é›†ç¾¤æ¯” `kind` ç¨å¾®å¤šä¸€äº›é™åˆ¶ï¼š

* 2 CPUs or more
* 2GB of free memory
* 20GB of free disk space

å¦åˆ™ä¼šæŠ¥ä¸‹é¢è¿™äº›é”™è¯¯ã€‚

CPU æ ¸æ•°ä¸å¤Ÿï¼š

```
X Exiting due to RSRC_INSUFFICIENT_CORES: Requested cpu count 2 is greater than the available cpus of 1
```

å†…å­˜ä¸å¤Ÿï¼š

```
X Exiting due to RSRC_INSUFFICIENT_CONTAINER_MEMORY: docker only has 990MiB available, less than the required 1800MiB for Kubernetes
```

å¦å¤–ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ Docker ä½œä¸ºé©±åŠ¨æ—¶ï¼Œéœ€è¦ä»¥é root ç”¨æˆ·è¿è¡Œï¼š

```
X Exiting due to DRV_AS_ROOT: The "docker" driver should not be used with root privileges.
```

Docker åœ¨å®‰è£…æ—¶ä¼šé»˜è®¤åˆ›å»ºä¸€ä¸ªå« `docker` çš„ç”¨æˆ·ç»„ï¼Œå¯ä»¥åœ¨ `/etc/group` æ–‡ä»¶ä¸­æ‰¾åˆ° `docker` ç”¨æˆ·ç»„çš„ idï¼Œç„¶åä½¿ç”¨ `adduser` åœ¨è¯¥ç”¨æˆ·ç»„ä¸‹æ·»åŠ ä¸€ä¸ª `docker` ç”¨æˆ·ï¼Œ`su - docker` åˆ‡æ¢åˆ° `docker` ç”¨æˆ·å°±å¯ä»¥ä»¥é root ç”¨æˆ·è¿è¡Œ Docker äº†ï¼š

```
[root@localhost ~]# grep docker /etc/group
docker:x:995:

[root@localhost ~]# adduser -g 995 -c "Docker" docker

[root@localhost ~]# id docker
uid=1000(docker) gid=995(docker) ç»„=995(docker)

[root@localhost ~]# su - docker
```

ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæ‰§è¡Œ `minikube start` åˆ›å»º Kubernetes é›†ç¾¤ï¼š

```
[docker@localhost ~]$ minikube start
* Centos 7.9.2009 ä¸Šçš„ minikube v1.25.2
* æ ¹æ®ç°æœ‰çš„é…ç½®æ–‡ä»¶ä½¿ç”¨ docker é©±åŠ¨ç¨‹åº
* Starting control plane node minikube in cluster minikube
* Pulling base image ...
    > index.docker.io/kicbase/sta...: 0 B [____________________] ?% ? p/s 6m29s
! minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.30, but successfully downloaded docker.io/kicbase/stable:v0.0.30 as a fallback image
* Creating docker container (CPUs=2, Memory=2200MB) ...

X Docker is nearly out of disk space, which may cause deployments to fail! (90% of capacity)
* å»ºè®®ï¼š

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
* Related issue: https://github.com/kubernetes/minikube/issues/9024

! This container is having trouble accessing https://k8s.gcr.io
* To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
* æ­£åœ¨ Docker 20.10.12 ä¸­å‡†å¤‡ Kubernetes v1.23.3â€¦
  - kubelet.housekeeping-interval=5m
  - Generating certificates and keys ...
  - Booting up control plane ...
  - Configuring RBAC rules ...
* Verifying Kubernetes components...
  - Using image gcr.io/k8s-minikube/storage-provisioner:v5
* Enabled addons: default-storageclass, storage-provisioner
* Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

ä½¿ç”¨ `docker ps` å¯ä»¥çœ‹åˆ° `minikube` ä½¿ç”¨ `kicbase/stable` é•œåƒå¯åŠ¨äº†ä¸€ä¸ªå®¹å™¨ï¼Œè¯¥å®¹å™¨æš´éœ²äº†ä»¥ä¸‹å‡ ä¸ªç«¯å£ï¼š

* 49157->22
* 49156->2376
* 49155->5000
* 49154->8443
* 49153->32443

```
[docker@localhost ~]$ docker ps -a
CONTAINER ID   IMAGE                    COMMAND                  CREATED         STATUS         PORTS                                                                                                                                  NAMES
d7e2ffaba188   kicbase/stable:v0.0.30   "/usr/local/bin/entrâ€¦"   2 minutes ago   Up 2 minutes   127.0.0.1:49157->22/tcp, 127.0.0.1:49156->2376/tcp, 127.0.0.1:49155->5000/tcp, 127.0.0.1:49154->8443/tcp, 127.0.0.1:49153->32443/tcp   minikube
```

æˆ‘ä»¬è¿›åˆ°å®¹å™¨å†…éƒ¨çœ‹çœ‹ï¼š

```
[docker@localhost ~]$ docker exec -it minikube bash
root@minikube:/# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.7  0.2  21848  8112 ?        Ss   00:44   0:01 /sbin/init
root         178  0.3  0.1  29028  5956 ?        S<s  00:44   0:00 /lib/systemd/systemd-journald
message+     189  0.0  0.0   6992  2048 ?        Ss   00:44   0:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activ
root         194  0.9  0.9 1493012 36696 ?       Ssl  00:44   0:01 /usr/bin/containerd
root         201  0.0  0.1  12168  3888 ?        Ss   00:44   0:00 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups
root         445  2.6  1.9 1900036 74280 ?       Ssl  00:44   0:04 /usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimi
root        1205  0.0  0.1 711432  6092 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id d7248cb46ce6675cd8571237b2d97b14
root        1234  0.0  0.1 711432  5916 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id c64eab39fcc84a16cf781946b19208a8
root        1235  0.0  0.1 711688  6052 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 160d78a5a6af0460766ea18b52712194
root        1248  0.0  0.1 711432  5540 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 60addc91e8a0ac5163c7aec249d4df17
root        1385  0.0  0.2 711176 10580 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id dacd9db0524cde32c07b69922e85eb22
root        1386  0.0  0.1 712840  6084 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 9d6e09b49fe389729643b4c000132fab
root        1426  0.0  0.1 712840  5892 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id c72e327a1759494f99936930c846abda
root        1439  0.0  0.1 711176  5880 ?        Sl   00:44   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 8ffdf3f55725c550e703a3d9f3d0f5b3
root        1458  3.2  0.8 754020 32528 ?        Ssl  00:44   0:04 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --authoriza
root        1477 14.6  8.0 1110392 312444 ?      Ssl  00:44   0:21 kube-apiserver --advertise-address=192.168.49.2 --allow-privileged=true --authorizati
root        1494  7.4  1.9 824644 76552 ?        Ssl  00:44   0:11 kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/etc/k
root        1506 12.1  0.9 11214516 38160 ?      Ssl  00:44   0:17 etcd --advertise-client-urls=https://192.168.49.2:2379 --cert-file=/var/lib/minikube/
root        1733  5.2  1.8 1862712 71784 ?       Ssl  00:45   0:06 /var/lib/minikube/binaries/v1.23.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/boo
root        1999  0.0  0.1 711688  7116 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 6bcfb5ef991c43859df52e82267a4ea2
root        2097  0.0  0.1 711432  5820 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id acee309420d41df02c11a0c5b581527e
root        2142  0.0  0.1 711432  5676 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 4c4d52d6bb2f9d2a1ca9f198c6d7e61f
root        2162  0.3  0.5 748424 21668 ?        Ssl  00:45   0:00 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-overrid
root        2195  0.0  0.1 710920  5932 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id c1fcfbe957680299873562cfb7d3d8a3
root        2337  0.0  0.1 711688  5708 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id d7dafa8e76578114f8aeaff1a4e6edd0
root        2357  0.5  0.6 750824 24468 ?        Ssl  00:45   0:00 /coredns -conf /etc/coredns/Corefile
root        2436  0.0  0.1 711432  5688 ?        Sl   00:45   0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 7d3a10b807c67b41603e66b2a1527e4d
root        2457  1.4  0.4 735712 16776 ?        Ssl  00:45   0:01 /storage-provisioner
root        2673  1.0  0.0   4236  2232 pts/1    Ss   00:47   0:00 bash
root        2687  0.0  0.0   5888  1520 pts/1    R+   00:47   0:00 ps aux
```

å¯ä»¥çœ‹åˆ°ä¸‹é¢è¿™äº›è¿›ç¨‹å’Œ `kind` ä¸€æ ·ï¼š

* containerd
* containerd-shim-runc-v2
* pause
* coredns
* etcd
* kubelet
* kube-apiserver
* kube-proxy
* kube-controller-manager
* kube-scheduler

ä¸‹é¢è¿™äº›è¿›ç¨‹ä¸ä¸€æ ·ï¼š

* dbus-daemon
* sshd
* dockerd
* storage-provisioner

å°‘äº†è¿™ä¸¤ä¸ªè¿›ç¨‹ï¼š

* kindnetd
* local-path-provisioner

## ä½¿ç”¨ kubeadm å®‰è£… Kubernetes

`kubeadm` æ˜¯ Kubernetes ç¤¾åŒºæä¾›çš„é›†ç¾¤æ„å»ºå·¥å…·ï¼Œå®ƒè´Ÿè´£æ„å»ºä¸€ä¸ªæœ€å°åŒ–å¯ç”¨é›†ç¾¤å¹¶æ‰§è¡Œå¯åŠ¨ç­‰å¿…è¦çš„åŸºæœ¬æ­¥éª¤ï¼Œç®€å•æ¥è®²ï¼Œ`kubeadm` æ˜¯ Kubernetes é›†ç¾¤å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å·¥å…·ï¼Œå¯ç”¨äºå®ç°é›†ç¾¤çš„éƒ¨ç½²ã€å‡çº§/é™çº§åŠå¸è½½ç­‰ã€‚æŒ‰ç…§è®¾è®¡ï¼Œå®ƒåªå…³æ³¨å¯åŠ¨å¼•å¯¼ï¼Œè€Œéé…ç½®æœºå™¨ã€‚åŒæ ·çš„ï¼Œå®‰è£…å„ç§ â€œé”¦ä¸Šæ·»èŠ±â€ çš„æ‰©å±•ï¼Œä¾‹å¦‚ Kubernetes Dashboardã€ç›‘æ§æ–¹æ¡ˆã€ä»¥åŠç‰¹å®šäº‘å¹³å°çš„æ‰©å±•ï¼Œéƒ½ä¸åœ¨è®¨è®ºèŒƒå›´å†…ã€‚

### å®‰è£… kubeadmã€kubelet å’Œ kubectl

é¦–å…ˆæˆ‘ä»¬éœ€è¦å®‰è£…è¿™ä¸‰ä¸ªç»„ä»¶ï¼š

* `kubeadm` - ç”¨äºå¯åŠ¨é›†ç¾¤
* `kubelet` - è¿è¡Œåœ¨é›†ç¾¤ä¸­çš„æ¯ä¸€å°æœºå™¨ä¸Šï¼Œç”¨äºå¯åŠ¨ Pod å’Œ å®¹å™¨
* `kubectl` - ç”¨äºç®¡ç†é›†ç¾¤

è™½ç„¶å®˜æ–¹æä¾›äº† [yum å’Œ apt-get çš„å®‰è£…æ–¹å¼](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl)ï¼Œä½†æ˜¯è¿™é‡Œæˆ‘æ‰“ç®—æ‰‹å·¥å®‰è£…ä¸‹ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½çš„åŠ æ·±ç†è§£ã€‚

#### ä¸‹è½½ CNI æ’ä»¶

ç»å¤§å¤šæ•° Pod ç½‘ç»œéƒ½éœ€è¦ CNI æ’ä»¶ã€‚

```
[root@localhost ~]# mkdir -p /opt/cni/bin
[root@localhost ~]# curl -L "https://github.com/containernetworking/plugins/releases/download/v0.8.2/cni-plugins-linux-amd64-v0.8.2.tgz" | sudo tar -C /opt/cni/bin -xz
```

å¯ä»¥çœ‹åˆ°è¿™é‡Œæä¾›äº†å¾ˆå¤šä¸åŒçš„ CNI æ’ä»¶ï¼š

```
[root@localhost ~]# ls /opt/cni/bin/
bandwidth  bridge  dhcp  firewall  flannel  host-device  host-local  ipvlan  loopback  macvlan  portmap  ptp  sbr  static  tuning  vlan
```

#### å®‰è£… crictl

`crictl` æ˜¯ `CRI` å…¼å®¹çš„å®¹å™¨è¿è¡Œæ—¶å‘½ä»¤è¡Œæ¥å£ã€‚ä½ å¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æŸ¥å’Œè°ƒè¯• Kubernetes èŠ‚ç‚¹ä¸Šçš„å®¹å™¨è¿è¡Œæ—¶å’Œåº”ç”¨ç¨‹åºã€‚

```
[root@localhost ~]# curl -L "https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.22.0/crictl-v1.22.0-linux-amd64.tar.gz" | sudo tar -C /usr/local/bin -xz
```

ä½¿ç”¨ `crictl --help` æŸ¥çœ‹å¸®åŠ©ï¼š

```
[root@localhost ~]# crictl --help
NAME:
   crictl - client for CRI

USAGE:
   crictl [global options] command [command options] [arguments...]

VERSION:
   v1.22.0

COMMANDS:
   attach              Attach to a running container
   create              Create a new container
   exec                Run a command in a running container
   version             Display runtime version information
   images, image, img  List images
   inspect             Display the status of one or more containers
   inspecti            Return the status of one or more images
   imagefsinfo         Return image filesystem info
   inspectp            Display the status of one or more pods
   logs                Fetch the logs of a container
   port-forward        Forward local port to a pod
   ps                  List containers
   pull                Pull an image from a registry
   run                 Run a new container inside a sandbox
   runp                Run a new pod
   rm                  Remove one or more containers
   rmi                 Remove one or more images
   rmp                 Remove one or more pods
   pods                List pods
   start               Start one or more created containers
   info                Display information of the container runtime
   stop                Stop one or more running containers
   stopp               Stop one or more running pods
   update              Update one or more running containers
   config              Get and set crictl client configuration options
   stats               List container(s) resource usage statistics
   completion          Output shell completion code
   help, h             Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --config value, -c value            Location of the client config file. If not specified and the default does not exist, the program's directory is searched as well (default: "/etc/crictl.yaml") [$CRI_CONFIG_FILE]
   --debug, -D                         Enable debug mode (default: false)
   --image-endpoint value, -i value    Endpoint of CRI image manager service (default: uses 'runtime-endpoint' setting) [$IMAGE_SERVICE_ENDPOINT]
   --runtime-endpoint value, -r value  Endpoint of CRI container runtime service (default: uses in order the first successful one of [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock]). Default is now deprecated and the endpoint should be set instead. [$CONTAINER_RUNTIME_ENDPOINT]
   --timeout value, -t value           Timeout of connecting to the server in seconds (e.g. 2s, 20s.). 0 or less is set to default (default: 2s)
   --help, -h                          show help (default: false)
   --version, -v                       print the version (default: false)
```

ä¸è¿‡åœ¨æ‰§è¡Œ `crictl ps` çš„æ—¶å€™æŠ¥é”™äº†ï¼š

```
[root@localhost ~]# crictl ps
WARN[0000] runtime connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
ERRO[0002] connect endpoint 'unix:///var/run/dockershim.sock', make sure you are running as root and the endpoint has been started: context deadline exceeded 
WARN[0002] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
ERRO[0004] connect endpoint 'unix:///var/run/dockershim.sock', make sure you are running as root and the endpoint has been started: context deadline exceeded 
FATA[0004] listing containers: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService 
```

æˆ‘ä»¬æ£€æŸ¥ containerd çš„é…ç½®æ–‡ä»¶ `/etc/containerd/config.toml`ï¼š

```
[root@localhost ~]# cat /etc/containerd/config.toml
#   Copyright 2018-2020 Docker Inc.

#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at

#       http://www.apache.org/licenses/LICENSE-2.0

#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

disabled_plugins = ["cri"]

#root = "/var/lib/containerd"
#state = "/run/containerd"
#subreaper = true
#oom_score = 0

#[grpc]
#  address = "/run/containerd/containerd.sock"
#  uid = 0
#  gid = 0

#[debug]
#  address = "/run/containerd/debug.sock"
#  uid = 0
#  gid = 0
#  level = "info"
```

å‘ç°é‡Œé¢æœ‰ä¸€è¡Œ `disabled_plugins = ["cri"]`ï¼Œè¿™æ˜¯ Docker é»˜è®¤å®‰è£…æ—¶çš„é…ç½®ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªé…ç½®åˆ é™¤ï¼Œå¹¶é‡å¯ containerdï¼š

```
[root@localhost ~]# rm /etc/containerd/config.toml
[root@localhost ~]# systemctl restart containerd
```

#### å®‰è£… kubeadmã€kubelet å’Œ kubectl

```
[root@localhost ~]# cd /usr/local/bin
[root@localhost bin]# curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/v1.24.0/bin/linux/amd64/{kubeadm,kubelet,kubectl}
[root@localhost bin]# chmod +x {kubeadm,kubelet,kubectl}
```

è¿™ä¸‰ä¸ªç»„ä»¶å®‰è£…å¥½ä¹‹åï¼Œæˆ‘ä»¬éœ€è¦å°† `kubelet` æ·»åŠ åˆ° systemd æœåŠ¡ã€‚é¦–å…ˆç›´æ¥ä»å®˜æ–¹ä¸‹è½½æœåŠ¡å®šä¹‰çš„æ¨¡æ¿ï¼Œä¿®æ”¹å…¶ä¸­ kubelet çš„è·¯å¾„ï¼š

```
[root@localhost ~]# curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.4.0/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service

[Unit]
Description=kubelet: The Kubernetes Node Agent
Documentation=https://kubernetes.io/docs/home/
Wants=network-online.target
After=network-online.target

[Service]
ExecStart=/usr/local/bin/kubelet
Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
```

ç„¶åå†ä¸‹è½½ kubeadm çš„é…ç½®æ–‡ä»¶ï¼š

```
[root@localhost ~]# mkdir -p /etc/systemd/system/kubelet.service.d
[root@localhost ~]# curl -sSL "https://raw.githubusercontent.com/kubernetes/release/v0.4.0/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf" | sed "s:/usr/bin:/usr/local/bin:g" | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

æœ€åï¼Œå¯åŠ¨ `kubelet` æœåŠ¡ï¼š

```
[root@localhost ~]# systemctl enable --now kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
```

### ä½¿ç”¨ kubeadm åˆ›å»º Kubernetes é›†ç¾¤

æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨ `kubeadm init` æ¥åˆå§‹åŒ– Kubernetes é›†ç¾¤ï¼Œè¿™ä¸ªå‘½ä»¤çš„ä½œç”¨æ˜¯å¸®åŠ©ä½ å¯åŠ¨å’Œ master èŠ‚ç‚¹ç›¸å…³çš„ç»„ä»¶ï¼š`kube-apiserver`ã€`kube-controller-manager`ã€`kube-scheduler` å’Œ `etcd` ç­‰ã€‚åœ¨è¿è¡Œä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `kubeadm config images list` å‘½ä»¤æŸ¥çœ‹ä½¿ç”¨ kubeadm åˆ›å»º Kubernetes é›†ç¾¤æ‰€éœ€è¦çš„é•œåƒï¼š

```
[root@localhost ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.24.0
k8s.gcr.io/kube-controller-manager:v1.24.0
k8s.gcr.io/kube-scheduler:v1.24.0
k8s.gcr.io/kube-proxy:v1.24.0
k8s.gcr.io/pause:3.7
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/coredns/coredns:v1.8.6
```

ä½¿ç”¨ `kubeadm config images pull` æå‰å°†é•œåƒä¸‹è½½ä¸‹æ¥ï¼š

```
[root@localhost ~]# kubeadm config images pull
failed to pull image "k8s.gcr.io/kube-apiserver:v1.24.0": output: time="2022-05-15T12:18:29+08:00" level=fatal msg="pulling image: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.ImageService"
, error: exit status 1
To see the stack trace of this error execute with --v=5 or higher
```

æˆ‘ä»¬å‘ç°ä¸‹è½½é•œåƒæŠ¥é”™ï¼Œè¿™æ˜¯å› ä¸ºå›½å†…æ²¡åŠæ³•è®¿é—® `k8s.gcr.io`ï¼Œè€Œä¸”æ— è®ºæ˜¯åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ä»£ç†ï¼Œè¿˜æ˜¯ä¸º Docker Daemon è®¾ç½®ä»£ç†ï¼Œéƒ½ä¸èµ·ä½œç”¨ã€‚åæ¥æ‰æ„è¯†åˆ°ï¼Œ`kubeadm config images pull` å‘½ä»¤è²Œä¼¼ä¸èµ° docker æœåŠ¡ï¼Œè€Œæ˜¯ç›´æ¥è¯·æ±‚ containerd æœåŠ¡ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸º containerd æœåŠ¡è®¾ç½®ä»£ç†ï¼š

```
[root@localhost ~]# mkdir /etc/systemd/system/containerd.service.d
[root@localhost ~]# vi /etc/systemd/system/containerd.service.d/http_proxy.conf
```

æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```
[Service]
Environment="HTTP_PROXY=192.168.1.36:10809"
Environment="HTTPS_PROXY=192.168.1.36:10809"
```

é‡å¯ containerd æœåŠ¡ï¼š

```
[root@localhost ~]# systemctl daemon-reload
[root@localhost ~]# systemctl restart containerd
```

ç„¶åé‡æ–°ä¸‹è½½é•œåƒï¼š

```
[root@localhost ~]# kubeadm config images pull
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.24.0
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.24.0
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.24.0
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.24.0
[config/images] Pulled k8s.gcr.io/pause:3.7
[config/images] Pulled k8s.gcr.io/etcd:3.5.3-0
[config/images] Pulled k8s.gcr.io/coredns/coredns:v1.8.6
```

æ¥ä¸‹æ¥ä½¿ç”¨ `kubeadm init` åˆå§‹åŒ– Kubernetes çš„æ§åˆ¶å¹³é¢ï¼š

```
[root@localhost ~]# kubeadm init
W0515 14:36:22.763487   21958 version.go:103] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get "https://dl.k8s.io/release/stable-1.txt": x509: certificate has expired or is not yet valid: current time 2022-05-15T14:36:22+08:00 is before 2022-05-17T21:21:32Z
W0515 14:36:22.763520   21958 version.go:104] falling back to the local client version: v1.24.0
[init] Using Kubernetes version: v1.24.0
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local localhost.localdomain] and IPs [10.96.0.1 10.0.2.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [10.0.2.10 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [10.0.2.10 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp [::1]:10248: connect: connection refused.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
```

æ ¹æ®æŠ¥é”™ä¿¡æ¯ï¼Œåº”è¯¥æ˜¯ `swap` çš„é—®é¢˜ï¼Œé€šè¿‡ä¸‹é¢çš„å‘½ä»¤å…³é—­ `swap`ï¼š

```
[root@localhost ~]# swapoff  -a
```

ç„¶åé‡æ–°æ‰§è¡Œ `kubeadm init`ï¼Œæ³¨æ„è¦å…ˆæ‰§è¡Œ `kubeadm reset`ï¼š

```
[root@localhost ~]# kubeadm reset
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0515 15:03:11.771080   25796 reset.go:103] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get "https://10.0.2.10:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 10.0.2.10:6443: connect: connection refused
W0515 15:03:11.773814   25796 preflight.go:55] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0515 15:03:13.272040   25796 removeetcdmember.go:84] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
```

å†æ¬¡æ‰§è¡Œ `kubeadm init` æˆåŠŸï¼š

```
[root@localhost ~]# kubeadm init
W0515 15:03:21.229843   25821 version.go:103] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get "https://dl.k8s.io/release/stable-1.txt": x509: certificate has expired or is not yet valid: current time 2022-05-15T15:03:21+08:00 is before 2022-05-17T21:21:32Z
W0515 15:03:21.229869   25821 version.go:104] falling back to the local client version: v1.24.0
[init] Using Kubernetes version: v1.24.0
[preflight] Running pre-flight checks
	[WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local localhost.localdomain] and IPs [10.96.0.1 10.0.2.10]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [10.0.2.10 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [10.0.2.10 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 22.259518 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: cjpeqg.yvf2lka5i5epqcis
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.2.10:6443 --token cjpeqg.yvf2lka5i5epqcis \
	--discovery-token-ca-cert-hash sha256:2c662bccbb9491d97b141a2b4b578867f240614ddcc399949c803d1f5093bba5 
```

æ ¹æ®æç¤ºï¼Œæˆ‘ä»¬å°†é…ç½®æ–‡ä»¶å¤åˆ¶åˆ° `~/.kube` ç›®å½•ï¼š

```
[root@localhost ~]# mkdir -p $HOME/.kube
[root@localhost ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@localhost ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

ç„¶åå®‰è£…ä¸€ä¸ª [Pod ç½‘ç»œæ’ä»¶](https://kubernetes.io/docs/concepts/cluster-administration/addons/)ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©å®‰è£… `flannel`ï¼š

```
[root@localhost ~]# curl -LO https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

[root@localhost ~]# kubectl apply -f kube-flannel.yml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

ç„¶ååœ¨å¦ä¸€å°æœºå™¨ä¸Šæ‰§è¡Œ `kubeadm join` å°†å·¥ä½œèŠ‚ç‚¹åŠ å…¥ Kubernetes é›†ç¾¤ï¼ˆè¿™å°æœºå™¨ä¹Ÿéœ€è¦æå‰å®‰è£…å¥½ kubeadmï¼‰ï¼š

```
[root@localhost ~]# kubeadm join 10.0.2.10:6443 --token cjpeqg.yvf2lka5i5epqcis \
	--discovery-token-ca-cert-hash sha256:2c662bccbb9491d97b141a2b4b578867f240614ddcc399949c803d1f5093bba5 
```

## å…¶ä»–å®‰è£…æˆ–ç®¡ç† Kubernetes çš„å·¥å…·

* [Kubernetes Dashboard](https://github.com/kubernetes/dashboard)
* [sealos](https://github.com/labring/sealos)
* [Rancher](https://rancher.com/quick-start)
* [Kuboard](https://kuboard.cn/)
* [Kubernetes Web View](https://kube-web-view.readthedocs.io/en/latest/index.html)

## å‚è€ƒ

1. [kubectl å®‰è£…æ–‡æ¡£](https://kubernetes.io/docs/reference/kubectl/)
1. [kind å®˜æ–¹æ–‡æ¡£](https://kind.sigs.k8s.io/docs/user/quick-start/)
1. [kindï¼šKubernetes in Dockerï¼Œå•æœºè¿è¡Œ Kubernetes ç¾¤é›†çš„æœ€ä½³æ–¹æ¡ˆ](https://sysin.org/blog/kind/)
1. [minikube å®˜æ–¹æ–‡æ¡£](https://minikube.sigs.k8s.io/docs/start/)
1. [Bootstrapping clusters with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/)
1. [ä¸€æ–‡ææ‡‚å®¹å™¨è¿è¡Œæ—¶ Containerd](https://www.qikqiak.com/post/containerd-usage/)

## æ›´å¤š

### 1. ä¸º Docker è®¾ç½®ä»£ç†

ç¬¬ä¸€ç§æƒ…å†µæ˜¯ [ä¸º Docker Daemon è®¾ç½®ä»£ç†](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy)ï¼Œå½±å“ docker pull ä¸‹è½½é•œåƒã€‚é¦–å…ˆåˆ›å»ºå¦‚ä¸‹ç›®å½•ï¼š

```
[root@localhost ~]# mkdir -p /etc/systemd/system/docker.service.d
```

åœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶ `http-proxy.conf`ï¼š

```
[root@localhost ~]# cd /etc/systemd/system/docker.service.d
[root@localhost docker.service.d]# vi http-proxy.conf
```

æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```
[Service]
Environment="HTTP_PROXY=192.168.1.36:10809"
Environment="HTTPS_PROXY=192.168.1.36:10809"
```

é‡å¯ Docker æœåŠ¡ï¼š

```
[root@localhost ~]# systemctl daemon-reload
[root@localhost ~]# systemctl restart docker
```

éªŒè¯ä»£ç†è®¾ç½®æ˜¯å¦ç”Ÿæ•ˆï¼š

```
[root@localhost ~]# systemctl show --property=Environment docker
Environment=HTTP_PROXY=192.168.1.36:10809 HTTPS_PROXY=192.168.1.36:10809
```

ç¬¬äºŒç§æƒ…å†µæ˜¯ [ä¸º Docker å®¹å™¨è®¾ç½®ä»£ç†](https://docs.docker.com/network/proxy/)ï¼Œå½±å“å®¹å™¨å†…è®¿é—®å¤–éƒ¨ç½‘ç»œã€‚è¿™ä¸ªé…ç½®æ¯”è¾ƒç®€å•ï¼Œåªéœ€è¦åœ¨ç”¨æˆ·ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª `~/.docker/config.json` æ–‡ä»¶ï¼š

```
[root@localhost ~]# mkdir -p ~/.docker
[root@localhost ~]# vi ~/.docker/config.json
```

æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š

```
{
  "proxies":
  {
    "default":
    {
      "httpProxy": "192.168.1.36:10809",
      "httpsProxy": "192.168.1.36:10809"
    }
  }
}
```

ä½¿ç”¨ `alpine/curl` é•œåƒå¯åŠ¨ä¸€ä¸ªå®¹å™¨ï¼ŒéªŒè¯é…ç½®æ˜¯å¦ç”Ÿæ•ˆï¼š

```
[root@localhost ~]# docker run --rm alpine/curl -fsSL ifconfig.me
103.168.154.81
```
